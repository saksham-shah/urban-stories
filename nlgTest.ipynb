{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlgTest.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOlA516HsRFIda677Gzz1wB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saksham-shah/urban-stories/blob/main/nlgTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "95IwK0GeCrdc"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pickle\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "2SspLbN6tEhG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read pickle file\n",
        "import pickle\n",
        "with open('plots_text.pickle','rb') as handle:\n",
        "  movie_plots = pickle.load(handle)\n",
        "\n",
        "# count of movie plot summaries\n",
        "len(movie_plots)\n"
      ],
      "metadata": {
        "id": "B7AMw63kC9ck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e1ce0a0-c6b9-427b-c25b-8faeb80a1d89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pAndP = open('pride&prejudice.txt','r')\n",
        "text = pAndP.read()"
      ],
      "metadata": {
        "id": "1SD6Edn-Ci3-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create sequences of length 5 tokens\n",
        "def create_seq(text, seq_len = 5):\n",
        "    text = text.lower()\n",
        "    sequences = []\n",
        "    count = 0\n",
        "    # if the number of tokens in 'text' is greater than 5\n",
        "    if len(text.split()) > seq_len:\n",
        "      for i in range(seq_len, len(text.split())):\n",
        "        # select sequence of tokens\n",
        "        seq = text.split()[i-seq_len:i+1]\n",
        "        # add to the list\n",
        "        sequences.append(\" \".join(seq))\n",
        "        count += 1\n",
        "        if (count %997 ==0):\n",
        "          print(count)\n",
        "      return sequences\n",
        "\n",
        "    # if the number of tokens in 'text' is less than or equal to 5\n",
        "    else:\n",
        "      \n",
        "      return [text]"
      ],
      "metadata": {
        "id": "miZn6IXuDPxu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seqs = create_seq(text)\n",
        "# lol think this is very inefficient"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWcC-_ElDDlp",
        "outputId": "760d95c6-75ae-4d00-d734-9456f1834d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "997\n",
            "1994\n",
            "2991\n",
            "3988\n",
            "4985\n",
            "5982\n",
            "6979\n",
            "7976\n",
            "8973\n",
            "9970\n",
            "10967\n",
            "11964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "\n",
        "seqs = [create_seq(i) for i in movie_plots]\n",
        "\n",
        "# merge list-of-lists into a single list\n",
        "seqs = sum(seqs, [])\n",
        "\n",
        "# count of sequences\n",
        "len(seqs)\n",
        "'''"
      ],
      "metadata": {
        "id": "8KehDeyWDQOk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73256725-80a7-4d7e-cfb7-6cef17d85a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "154084"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "51IxSaI-J-re",
        "outputId": "23ce8bec-8203-4c8a-b0c1-18e788b851bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f58252b3ccab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mseqs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseqs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create inputs and targets (x and y)\n",
        "x = []\n",
        "y = []\n",
        "\n",
        "for s in seqs:\n",
        "  x.append(\" \".join(s.split()[:-1]))\n",
        "  y.append(\" \".join(s.split()[1:]))"
      ],
      "metadata": {
        "id": "FeJIBZYbDY-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create integer-to-token mapping\n",
        "int2token = {}\n",
        "cnt = 0\n",
        "\n",
        "for w in set(text.lower().split()):\n",
        "  int2token[cnt] = w\n",
        "  cnt+= 1\n",
        "\n",
        "# create token-to-integer mapping\n",
        "token2int = {t: i for i, t in int2token.items()}\n",
        "\n",
        "token2int[\"the\"], int2token[1471]"
      ],
      "metadata": {
        "id": "AubXgbOzDdvs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53e9d1bd-a041-420f-d6ab-cfeba2abd5bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11358, 'street;')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(int2token)\n",
        "len(x)\n"
      ],
      "metadata": {
        "id": "jxb6Pe2IQu50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb1056d3-4c25-4e73-907c-9d6b5cfbc03a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "124744"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_integer_seq(seq):\n",
        " \n",
        "  return [token2int[w] for w in seq.split()]\n",
        "\n",
        "# convert text sequences to integer sequences\n",
        "x_int = [get_integer_seq(i) for i in x]\n",
        "y_int = [get_integer_seq(i) for i in y]\n",
        "\n",
        "# convert lists to numpy arrays\n",
        "x_int = np.array(x_int)\n",
        "y_int = np.array(y_int)"
      ],
      "metadata": {
        "id": "6myk5Mi5DdyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(arr_x, arr_y, batch_size):\n",
        "         \n",
        "    # iterate through the arrays\n",
        "    prv = 0\n",
        "    for n in range(batch_size, arr_x.shape[0], batch_size):\n",
        "      x = arr_x[prv:n,:]\n",
        "      y = arr_y[prv:n,:]\n",
        "      prv = n\n",
        "      yield x, y"
      ],
      "metadata": {
        "id": "cDS5bvKVDd0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WordLSTM(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_hidden=256, n_layers=4, drop_prob=0.3, lr=0.001):\n",
        "        super().__init__()\n",
        "\n",
        "        self.drop_prob = drop_prob\n",
        "        self.n_layers = n_layers\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = lr\n",
        "        \n",
        "        self.emb_layer = nn.Embedding(vocab_size, 200)\n",
        "\n",
        "        ## define the LSTM\n",
        "        self.lstm = nn.LSTM(200, n_hidden, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        ## define a dropout layer\n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        ## define the fully-connected layer\n",
        "        self.fc = nn.Linear(n_hidden, vocab_size)      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        ''' Forward pass through the network. \n",
        "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
        "\n",
        "        ## pass input through embedding layer\n",
        "        embedded = self.emb_layer(x)     \n",
        "        \n",
        "        ## Get the outputs and the new hidden state from the lstm\n",
        "        lstm_output, hidden = self.lstm(embedded, hidden)\n",
        "        \n",
        "        ## pass through a dropout layer\n",
        "        out = self.dropout(lstm_output)\n",
        "        \n",
        "        #out = out.contiguous().view(-1, self.n_hidden) \n",
        "        out = out.reshape(-1, self.n_hidden) \n",
        "\n",
        "        ## put \"out\" through the fully-connected layer\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # return the final output and the hidden state\n",
        "        return out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        # if GPU is available\n",
        "        if (torch.cuda.is_available()):\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
        "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
        "        \n",
        "        # if GPU is not available\n",
        "        else:\n",
        "          hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
        "                    weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
        "        \n",
        "        return hidden"
      ],
      "metadata": {
        "id": "QlWpBGs6Dvue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the model\n",
        "net = WordLSTM()\n",
        "\n",
        "# push the model to GPU (avoid it if you are not using the GPU)\n",
        "net.cuda()\n",
        "\n",
        "print(net)"
      ],
      "metadata": {
        "id": "6OzFEZbPDyOX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcfc8d37-7ce4-4de1-929c-4c86c9fdb0ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WordLSTM(\n",
            "  (emb_layer): Embedding(13132, 200)\n",
            "  (lstm): LSTM(200, 256, num_layers=4, batch_first=True, dropout=0.3)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=13132, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, epochs=10, batch_size=32, lr=0.001, clip=1, print_every=32):\n",
        "    \n",
        "    # optimizer\n",
        "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "    \n",
        "    # loss\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # push model to GPU\n",
        "    net.cuda()\n",
        "    \n",
        "    counter = 0\n",
        "\n",
        "    net.train()\n",
        "\n",
        "    for e in range(epochs):\n",
        "\n",
        "        # initialize hidden state\n",
        "        h = net.init_hidden(batch_size)\n",
        "        \n",
        "        for x, y in get_batches(x_int, y_int, batch_size):\n",
        "            counter+= 1\n",
        "            \n",
        "            # convert numpy arrays to PyTorch arrays\n",
        "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
        "            \n",
        "            # push tensors to GPU\n",
        "            inputs, targets = inputs.cuda(), targets.cuda()\n",
        "\n",
        "            # detach hidden states\n",
        "            h = tuple([each.data for each in h])\n",
        "\n",
        "            # zero accumulated gradients\n",
        "            net.zero_grad()\n",
        "            \n",
        "            # get the output from the model\n",
        "            output, h = net(inputs, h)\n",
        "            \n",
        "            # calculate the loss and perform backprop\n",
        "            loss = criterion(output, targets.view(-1))\n",
        "\n",
        "            # back-propagate error\n",
        "            loss.backward()\n",
        "\n",
        "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "\n",
        "            # update weigths\n",
        "            opt.step()            \n",
        "            \n",
        "            if counter % print_every == 0:\n",
        "            \n",
        "              print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                    \"Step: {}...\".format(counter))"
      ],
      "metadata": {
        "id": "gKsStv0nD2Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict next token\n",
        "def predict(net, tkn, maxProb, targets, h=None):\n",
        "  targets = [token2int[token] for token in targets]       \n",
        "  # tensor inputs\n",
        "  x = np.array([[token2int[tkn]]])\n",
        "  inputs = torch.from_numpy(x)\n",
        "  \n",
        "  # push to GPU\n",
        "  inputs = inputs.cuda()\n",
        "\n",
        "  # detach hidden state from history\n",
        "  h = tuple([each.data for each in h])\n",
        "\n",
        "  # get the output of the model\n",
        "  out, h = net(inputs, h)\n",
        "\n",
        "  # get the token probabilities\n",
        "  p = F.softmax(out, dim=1).data\n",
        "\n",
        "  p = p.cpu()\n",
        "\n",
        "  p = p.numpy()\n",
        "  p = p.reshape(p.shape[1],)\n",
        "\n",
        "  # get indices of top 3 values\n",
        "  sampled = False\n",
        "  top_n_idx = p.argsort()[-3:][::-1]\n",
        "  if(targets):\n",
        "    for target in targets:\n",
        "      if(p[target]>maxProb):\n",
        "        sampled_token_index = target\n",
        "        sampled = True\n",
        "        break\n",
        "      else:\n",
        "        # randomly select one of the three indices\n",
        "        sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
        "  else:\n",
        "    \n",
        "    # randomly select one of the three indices\n",
        "    sampled_token_index = top_n_idx[random.sample([0,1,2],1)[0]]\n",
        "\n",
        "\n",
        "\n",
        "  # return the encoded value of the predicted char and the hidden state\n",
        "  return int2token[sampled_token_index], h,sampled\n",
        "\n",
        "\n",
        "\n",
        "# function to generate text\n",
        "def sample(net, size,targets, prime='it is'):\n",
        "        \n",
        "    # push to GPU\n",
        "    net.cuda()\n",
        "    \n",
        "    net.eval()\n",
        "\n",
        "    # batch size is 1\n",
        "    h = net.init_hidden(1)\n",
        "\n",
        "    toks = prime.split()\n",
        "    maxProb = 0.003; \n",
        "    # can do some optimising to find best maxProb here to get roughly one target per ~20 word sentence\n",
        "\n",
        "    # predict next token\n",
        "    for t in prime.split():\n",
        "      token, h ,inserted= predict(net, t,maxProb,targets, h)\n",
        "    \n",
        "    toks.append(token)\n",
        "    #reset = net.copy()\n",
        "    #if no target word in 15 recursively call sample with reset as net\n",
        "    # max recursion 10? times\n",
        "    #\n",
        "\n",
        "    # predict subsequent tokens\n",
        "    for i in range(size-1):\n",
        "        token, h, inserted = predict(net, toks[-1],maxProb, targets, h )\n",
        "        if (inserted):\n",
        "          targets.remove(token)\n",
        "        toks.append(token)\n",
        "        #print(targets,token)\n",
        "\n",
        "    return ' '.join(toks)"
      ],
      "metadata": {
        "id": "touR3EEuER3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(net, batch_size = 32, epochs=20, print_every=256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1PCx0xeTunA",
        "outputId": "c954ac58-a8d3-4c3d-d78a-b7bbbf6faba0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/20... Step: 256...\n",
            "Epoch: 1/20... Step: 512...\n",
            "Epoch: 1/20... Step: 768...\n",
            "Epoch: 1/20... Step: 1024...\n",
            "Epoch: 1/20... Step: 1280...\n",
            "Epoch: 1/20... Step: 1536...\n",
            "Epoch: 1/20... Step: 1792...\n",
            "Epoch: 1/20... Step: 2048...\n",
            "Epoch: 1/20... Step: 2304...\n",
            "Epoch: 1/20... Step: 2560...\n",
            "Epoch: 1/20... Step: 2816...\n",
            "Epoch: 1/20... Step: 3072...\n",
            "Epoch: 1/20... Step: 3328...\n",
            "Epoch: 1/20... Step: 3584...\n",
            "Epoch: 1/20... Step: 3840...\n",
            "Epoch: 2/20... Step: 4096...\n",
            "Epoch: 2/20... Step: 4352...\n",
            "Epoch: 2/20... Step: 4608...\n",
            "Epoch: 2/20... Step: 4864...\n",
            "Epoch: 2/20... Step: 5120...\n",
            "Epoch: 2/20... Step: 5376...\n",
            "Epoch: 2/20... Step: 5632...\n",
            "Epoch: 2/20... Step: 5888...\n",
            "Epoch: 2/20... Step: 6144...\n",
            "Epoch: 2/20... Step: 6400...\n",
            "Epoch: 2/20... Step: 6656...\n",
            "Epoch: 2/20... Step: 6912...\n",
            "Epoch: 2/20... Step: 7168...\n",
            "Epoch: 2/20... Step: 7424...\n",
            "Epoch: 2/20... Step: 7680...\n",
            "Epoch: 3/20... Step: 7936...\n",
            "Epoch: 3/20... Step: 8192...\n",
            "Epoch: 3/20... Step: 8448...\n",
            "Epoch: 3/20... Step: 8704...\n",
            "Epoch: 3/20... Step: 8960...\n",
            "Epoch: 3/20... Step: 9216...\n",
            "Epoch: 3/20... Step: 9472...\n",
            "Epoch: 3/20... Step: 9728...\n",
            "Epoch: 3/20... Step: 9984...\n",
            "Epoch: 3/20... Step: 10240...\n",
            "Epoch: 3/20... Step: 10496...\n",
            "Epoch: 3/20... Step: 10752...\n",
            "Epoch: 3/20... Step: 11008...\n",
            "Epoch: 3/20... Step: 11264...\n",
            "Epoch: 3/20... Step: 11520...\n",
            "Epoch: 4/20... Step: 11776...\n",
            "Epoch: 4/20... Step: 12032...\n",
            "Epoch: 4/20... Step: 12288...\n",
            "Epoch: 4/20... Step: 12544...\n",
            "Epoch: 4/20... Step: 12800...\n",
            "Epoch: 4/20... Step: 13056...\n",
            "Epoch: 4/20... Step: 13312...\n",
            "Epoch: 4/20... Step: 13568...\n",
            "Epoch: 4/20... Step: 13824...\n",
            "Epoch: 4/20... Step: 14080...\n",
            "Epoch: 4/20... Step: 14336...\n",
            "Epoch: 4/20... Step: 14592...\n",
            "Epoch: 4/20... Step: 14848...\n",
            "Epoch: 4/20... Step: 15104...\n",
            "Epoch: 4/20... Step: 15360...\n",
            "Epoch: 5/20... Step: 15616...\n",
            "Epoch: 5/20... Step: 15872...\n",
            "Epoch: 5/20... Step: 16128...\n",
            "Epoch: 5/20... Step: 16384...\n",
            "Epoch: 5/20... Step: 16640...\n",
            "Epoch: 5/20... Step: 16896...\n",
            "Epoch: 5/20... Step: 17152...\n",
            "Epoch: 5/20... Step: 17408...\n",
            "Epoch: 5/20... Step: 17664...\n",
            "Epoch: 5/20... Step: 17920...\n",
            "Epoch: 5/20... Step: 18176...\n",
            "Epoch: 5/20... Step: 18432...\n",
            "Epoch: 5/20... Step: 18688...\n",
            "Epoch: 5/20... Step: 18944...\n",
            "Epoch: 5/20... Step: 19200...\n",
            "Epoch: 5/20... Step: 19456...\n",
            "Epoch: 6/20... Step: 19712...\n",
            "Epoch: 6/20... Step: 19968...\n",
            "Epoch: 6/20... Step: 20224...\n",
            "Epoch: 6/20... Step: 20480...\n",
            "Epoch: 6/20... Step: 20736...\n",
            "Epoch: 6/20... Step: 20992...\n",
            "Epoch: 6/20... Step: 21248...\n",
            "Epoch: 6/20... Step: 21504...\n",
            "Epoch: 6/20... Step: 21760...\n",
            "Epoch: 6/20... Step: 22016...\n",
            "Epoch: 6/20... Step: 22272...\n",
            "Epoch: 6/20... Step: 22528...\n",
            "Epoch: 6/20... Step: 22784...\n",
            "Epoch: 6/20... Step: 23040...\n",
            "Epoch: 6/20... Step: 23296...\n",
            "Epoch: 7/20... Step: 23552...\n",
            "Epoch: 7/20... Step: 23808...\n",
            "Epoch: 7/20... Step: 24064...\n",
            "Epoch: 7/20... Step: 24320...\n",
            "Epoch: 7/20... Step: 24576...\n",
            "Epoch: 7/20... Step: 24832...\n",
            "Epoch: 7/20... Step: 25088...\n",
            "Epoch: 7/20... Step: 25344...\n",
            "Epoch: 7/20... Step: 25600...\n",
            "Epoch: 7/20... Step: 25856...\n",
            "Epoch: 7/20... Step: 26112...\n",
            "Epoch: 7/20... Step: 26368...\n",
            "Epoch: 7/20... Step: 26624...\n",
            "Epoch: 7/20... Step: 26880...\n",
            "Epoch: 7/20... Step: 27136...\n",
            "Epoch: 8/20... Step: 27392...\n",
            "Epoch: 8/20... Step: 27648...\n",
            "Epoch: 8/20... Step: 27904...\n",
            "Epoch: 8/20... Step: 28160...\n",
            "Epoch: 8/20... Step: 28416...\n",
            "Epoch: 8/20... Step: 28672...\n",
            "Epoch: 8/20... Step: 28928...\n",
            "Epoch: 8/20... Step: 29184...\n",
            "Epoch: 8/20... Step: 29440...\n",
            "Epoch: 8/20... Step: 29696...\n",
            "Epoch: 8/20... Step: 29952...\n",
            "Epoch: 8/20... Step: 30208...\n",
            "Epoch: 8/20... Step: 30464...\n",
            "Epoch: 8/20... Step: 30720...\n",
            "Epoch: 8/20... Step: 30976...\n",
            "Epoch: 9/20... Step: 31232...\n",
            "Epoch: 9/20... Step: 31488...\n",
            "Epoch: 9/20... Step: 31744...\n",
            "Epoch: 9/20... Step: 32000...\n",
            "Epoch: 9/20... Step: 32256...\n",
            "Epoch: 9/20... Step: 32512...\n",
            "Epoch: 9/20... Step: 32768...\n",
            "Epoch: 9/20... Step: 33024...\n",
            "Epoch: 9/20... Step: 33280...\n",
            "Epoch: 9/20... Step: 33536...\n",
            "Epoch: 9/20... Step: 33792...\n",
            "Epoch: 9/20... Step: 34048...\n",
            "Epoch: 9/20... Step: 34304...\n",
            "Epoch: 9/20... Step: 34560...\n",
            "Epoch: 9/20... Step: 34816...\n",
            "Epoch: 9/20... Step: 35072...\n",
            "Epoch: 10/20... Step: 35328...\n",
            "Epoch: 10/20... Step: 35584...\n",
            "Epoch: 10/20... Step: 35840...\n",
            "Epoch: 10/20... Step: 36096...\n",
            "Epoch: 10/20... Step: 36352...\n",
            "Epoch: 10/20... Step: 36608...\n",
            "Epoch: 10/20... Step: 36864...\n",
            "Epoch: 10/20... Step: 37120...\n",
            "Epoch: 10/20... Step: 37376...\n",
            "Epoch: 10/20... Step: 37632...\n",
            "Epoch: 10/20... Step: 37888...\n",
            "Epoch: 10/20... Step: 38144...\n",
            "Epoch: 10/20... Step: 38400...\n",
            "Epoch: 10/20... Step: 38656...\n",
            "Epoch: 10/20... Step: 38912...\n",
            "Epoch: 11/20... Step: 39168...\n",
            "Epoch: 11/20... Step: 39424...\n",
            "Epoch: 11/20... Step: 39680...\n",
            "Epoch: 11/20... Step: 39936...\n",
            "Epoch: 11/20... Step: 40192...\n",
            "Epoch: 11/20... Step: 40448...\n",
            "Epoch: 11/20... Step: 40704...\n",
            "Epoch: 11/20... Step: 40960...\n",
            "Epoch: 11/20... Step: 41216...\n",
            "Epoch: 11/20... Step: 41472...\n",
            "Epoch: 11/20... Step: 41728...\n",
            "Epoch: 11/20... Step: 41984...\n",
            "Epoch: 11/20... Step: 42240...\n",
            "Epoch: 11/20... Step: 42496...\n",
            "Epoch: 11/20... Step: 42752...\n",
            "Epoch: 12/20... Step: 43008...\n",
            "Epoch: 12/20... Step: 43264...\n",
            "Epoch: 12/20... Step: 43520...\n",
            "Epoch: 12/20... Step: 43776...\n",
            "Epoch: 12/20... Step: 44032...\n",
            "Epoch: 12/20... Step: 44288...\n",
            "Epoch: 12/20... Step: 44544...\n",
            "Epoch: 12/20... Step: 44800...\n",
            "Epoch: 12/20... Step: 45056...\n",
            "Epoch: 12/20... Step: 45312...\n",
            "Epoch: 12/20... Step: 45568...\n",
            "Epoch: 12/20... Step: 45824...\n",
            "Epoch: 12/20... Step: 46080...\n",
            "Epoch: 12/20... Step: 46336...\n",
            "Epoch: 12/20... Step: 46592...\n",
            "Epoch: 13/20... Step: 46848...\n",
            "Epoch: 13/20... Step: 47104...\n",
            "Epoch: 13/20... Step: 47360...\n",
            "Epoch: 13/20... Step: 47616...\n",
            "Epoch: 13/20... Step: 47872...\n",
            "Epoch: 13/20... Step: 48128...\n",
            "Epoch: 13/20... Step: 48384...\n",
            "Epoch: 13/20... Step: 48640...\n",
            "Epoch: 13/20... Step: 48896...\n",
            "Epoch: 13/20... Step: 49152...\n",
            "Epoch: 13/20... Step: 49408...\n",
            "Epoch: 13/20... Step: 49664...\n",
            "Epoch: 13/20... Step: 49920...\n",
            "Epoch: 13/20... Step: 50176...\n",
            "Epoch: 13/20... Step: 50432...\n",
            "Epoch: 14/20... Step: 50688...\n",
            "Epoch: 14/20... Step: 50944...\n",
            "Epoch: 14/20... Step: 51200...\n",
            "Epoch: 14/20... Step: 51456...\n",
            "Epoch: 14/20... Step: 51712...\n",
            "Epoch: 14/20... Step: 51968...\n",
            "Epoch: 14/20... Step: 52224...\n",
            "Epoch: 14/20... Step: 52480...\n",
            "Epoch: 14/20... Step: 52736...\n",
            "Epoch: 14/20... Step: 52992...\n",
            "Epoch: 14/20... Step: 53248...\n",
            "Epoch: 14/20... Step: 53504...\n",
            "Epoch: 14/20... Step: 53760...\n",
            "Epoch: 14/20... Step: 54016...\n",
            "Epoch: 14/20... Step: 54272...\n",
            "Epoch: 14/20... Step: 54528...\n",
            "Epoch: 15/20... Step: 54784...\n",
            "Epoch: 15/20... Step: 55040...\n",
            "Epoch: 15/20... Step: 55296...\n",
            "Epoch: 15/20... Step: 55552...\n",
            "Epoch: 15/20... Step: 55808...\n",
            "Epoch: 15/20... Step: 56064...\n",
            "Epoch: 15/20... Step: 56320...\n",
            "Epoch: 15/20... Step: 56576...\n",
            "Epoch: 15/20... Step: 56832...\n",
            "Epoch: 15/20... Step: 57088...\n",
            "Epoch: 15/20... Step: 57344...\n",
            "Epoch: 15/20... Step: 57600...\n",
            "Epoch: 15/20... Step: 57856...\n",
            "Epoch: 15/20... Step: 58112...\n",
            "Epoch: 15/20... Step: 58368...\n",
            "Epoch: 16/20... Step: 58624...\n",
            "Epoch: 16/20... Step: 58880...\n",
            "Epoch: 16/20... Step: 59136...\n",
            "Epoch: 16/20... Step: 59392...\n",
            "Epoch: 16/20... Step: 59648...\n",
            "Epoch: 16/20... Step: 59904...\n",
            "Epoch: 16/20... Step: 60160...\n",
            "Epoch: 16/20... Step: 60416...\n",
            "Epoch: 16/20... Step: 60672...\n",
            "Epoch: 16/20... Step: 60928...\n",
            "Epoch: 16/20... Step: 61184...\n",
            "Epoch: 16/20... Step: 61440...\n",
            "Epoch: 16/20... Step: 61696...\n",
            "Epoch: 16/20... Step: 61952...\n",
            "Epoch: 16/20... Step: 62208...\n",
            "Epoch: 17/20... Step: 62464...\n",
            "Epoch: 17/20... Step: 62720...\n",
            "Epoch: 17/20... Step: 62976...\n",
            "Epoch: 17/20... Step: 63232...\n",
            "Epoch: 17/20... Step: 63488...\n",
            "Epoch: 17/20... Step: 63744...\n",
            "Epoch: 17/20... Step: 64000...\n",
            "Epoch: 17/20... Step: 64256...\n",
            "Epoch: 17/20... Step: 64512...\n",
            "Epoch: 17/20... Step: 64768...\n",
            "Epoch: 17/20... Step: 65024...\n",
            "Epoch: 17/20... Step: 65280...\n",
            "Epoch: 17/20... Step: 65536...\n",
            "Epoch: 17/20... Step: 65792...\n",
            "Epoch: 17/20... Step: 66048...\n",
            "Epoch: 18/20... Step: 66304...\n",
            "Epoch: 18/20... Step: 66560...\n",
            "Epoch: 18/20... Step: 66816...\n",
            "Epoch: 18/20... Step: 67072...\n",
            "Epoch: 18/20... Step: 67328...\n",
            "Epoch: 18/20... Step: 67584...\n",
            "Epoch: 18/20... Step: 67840...\n",
            "Epoch: 18/20... Step: 68096...\n",
            "Epoch: 18/20... Step: 68352...\n",
            "Epoch: 18/20... Step: 68608...\n",
            "Epoch: 18/20... Step: 68864...\n",
            "Epoch: 18/20... Step: 69120...\n",
            "Epoch: 18/20... Step: 69376...\n",
            "Epoch: 18/20... Step: 69632...\n",
            "Epoch: 18/20... Step: 69888...\n",
            "Epoch: 18/20... Step: 70144...\n",
            "Epoch: 19/20... Step: 70400...\n",
            "Epoch: 19/20... Step: 70656...\n",
            "Epoch: 19/20... Step: 70912...\n",
            "Epoch: 19/20... Step: 71168...\n",
            "Epoch: 19/20... Step: 71424...\n",
            "Epoch: 19/20... Step: 71680...\n",
            "Epoch: 19/20... Step: 71936...\n",
            "Epoch: 19/20... Step: 72192...\n",
            "Epoch: 19/20... Step: 72448...\n",
            "Epoch: 19/20... Step: 72704...\n",
            "Epoch: 19/20... Step: 72960...\n",
            "Epoch: 19/20... Step: 73216...\n",
            "Epoch: 19/20... Step: 73472...\n",
            "Epoch: 19/20... Step: 73728...\n",
            "Epoch: 19/20... Step: 73984...\n",
            "Epoch: 20/20... Step: 74240...\n",
            "Epoch: 20/20... Step: 74496...\n",
            "Epoch: 20/20... Step: 74752...\n",
            "Epoch: 20/20... Step: 75008...\n",
            "Epoch: 20/20... Step: 75264...\n",
            "Epoch: 20/20... Step: 75520...\n",
            "Epoch: 20/20... Step: 75776...\n",
            "Epoch: 20/20... Step: 76032...\n",
            "Epoch: 20/20... Step: 76288...\n",
            "Epoch: 20/20... Step: 76544...\n",
            "Epoch: 20/20... Step: 76800...\n",
            "Epoch: 20/20... Step: 77056...\n",
            "Epoch: 20/20... Step: 77312...\n",
            "Epoch: 20/20... Step: 77568...\n",
            "Epoch: 20/20... Step: 77824...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample(net,2000,[\"turn\",\"love\",\"darcy\"],\"the\")"
      ],
      "metadata": {
        "id": "x5RcF-cGQzmy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "442292f6-0ba5-4a1b-ddae-41c09be72319"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bf764ce3f2d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"turn\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"love\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"darcy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"the\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sample' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('trainedNet.pickle','rb') as handle:\n",
        "  net = pickle.load(handle)"
      ],
      "metadata": {
        "id": "N3Vq61QmTt7U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "14b9ec24-773b-42c5-cf60-370cfbc5fa5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-2c6f97719684>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'trainedNet.pickle'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '5'."
          ]
        }
      ]
    }
  ]
}